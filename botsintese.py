#!/usr/bin/env python3
"""
BotS√≠ntese v3.0 - S√≠ntese Processual Automatizada
Extra√ß√£o, organiza√ß√£o e s√≠ntese factual de processos judiciais.

Suporta:
- Processamento local (Ollama/Llama)
- Cloud: Google Gemini, Anthropic Claude, OpenAI GPT, xAI Grok

Autor: Gerado por Claude (Anthropic)
Vers√£o: 2.0.0
"""

import os
import sys
import re
import json
import time
import hashlib
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass, field
from typing import Optional, List, Dict, Tuple
import tkinter as tk
from tkinter import filedialog, messagebox, ttk
import threading

# Depend√™ncias externas
try:
    import requests
    import yaml
    from PyPDF2 import PdfReader
    from docx import Document
    from docx.shared import Pt, Inches, Cm
    from docx.enum.text import WD_ALIGN_PARAGRAPH
    from docx.enum.table import WD_TABLE_ALIGNMENT
except ImportError as e:
    print(f"Erro: Depend√™ncia n√£o encontrada - {e}")
    print("Execute: pip install requests pyyaml PyPDF2 python-docx --break-system-packages")
    sys.exit(1)

# ============================================================================
# CONFIGURA√á√ïES
# ============================================================================

@dataclass
class Config:
    """Configura√ß√µes do BotS√≠ntese"""
    # APIs Cloud
    api_anthropic: str = ""
    api_openai: str = ""
    api_google: str = ""
    api_xai: str = ""
    
    # Ollama (local)
    ollama_host: str = "http://localhost:11434"
    modelo_local: str = "llama3.1:8b-instruct-q4_K_M"
    
    # Modo padr√£o
    modo_padrao: str = "google"  # local, google, anthropic, openai, xai
    
    # Processamento - chunks maiores para aproveitar contexto do Gemini
    chunk_size_local: int = 6000   # ~24k chars para Llama 8B
    chunk_size_cloud: int = 50000  # ~200k chars para Gemini (tem 1M contexto)
    chars_per_token: float = 4.0


def carregar_config(pasta_script: Path) -> Config:
    """Carrega configura√ß√µes do arquivo YAML"""
    config = Config()
    
    # Procura config na pasta do script
    config_file = pasta_script / "botsintese_config.yaml"
    
    if config_file.exists():
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                dados = yaml.safe_load(f) or {}
                
            # APIs
            apis = dados.get('apis', {})
            config.api_anthropic = apis.get('anthropic', '')
            config.api_openai = apis.get('openai', '')
            config.api_google = apis.get('google', '')
            config.api_xai = apis.get('xai', '')
            
            # Ollama
            ollama = dados.get('ollama', {})
            config.ollama_host = ollama.get('host', config.ollama_host)
            config.modelo_local = ollama.get('modelo', config.modelo_local)
            
            # Geral
            config.modo_padrao = dados.get('modo_padrao', 'local')
            
        except Exception as e:
            print(f"Aviso: Erro ao carregar config: {e}")
    
    return config


def salvar_config(config: Config, pasta_script: Path):
    """Salva configura√ß√µes no arquivo YAML"""
    config_file = pasta_script / "botsintese_config.yaml"
    
    dados = {
        'apis': {
            'anthropic': config.api_anthropic,
            'openai': config.api_openai,
            'google': config.api_google,
            'xai': config.api_xai,
        },
        'ollama': {
            'host': config.ollama_host,
            'modelo': config.modelo_local,
        },
        'modo_padrao': config.modo_padrao,
    }
    
    with open(config_file, 'w', encoding='utf-8') as f:
        yaml.dump(dados, f, allow_unicode=True, default_flow_style=False)


# ============================================================================
# DETEC√á√ÉO DE SISTEMA PROCESSUAL
# ============================================================================

@dataclass
class EventoProcessual:
    """Representa um evento/documento do processo"""
    data: str
    tipo: str
    descricao: str
    pagina_inicio: Optional[int] = None
    pagina_fim: Optional[int] = None
    conteudo: str = ""


@dataclass
class DadosProcesso:
    """Dados extra√≠dos do processo"""
    numero: str = ""
    classe: str = ""
    vara: str = ""
    comarca: str = ""
    valor_causa: str = ""
    data_distribuicao: str = ""
    assunto: str = ""
    
    partes: List[Dict] = field(default_factory=list)
    eventos: List[EventoProcessual] = field(default_factory=list)
    
    sistema: str = "generico"  # pje, eproc, saj, projudi, generico


def detectar_sistema(texto: str) -> str:
    """Detecta qual sistema processual gerou o PDF"""
    texto_lower = texto[:5000].lower()
    
    if "pje - processo judicial eletr√¥nico" in texto_lower or "pje.tjmg" in texto_lower:
        return "pje"
    elif "p√°gina de separa√ß√£o" in texto_lower and "evento" in texto_lower:
        return "eproc"
    elif "projudi" in texto_lower:
        return "projudi"
    elif "saj" in texto_lower or "esaj" in texto_lower:
        return "saj"
    else:
        return "generico"


def extrair_dados_pje(texto: str) -> DadosProcesso:
    """Extrai dados de PDF do sistema PJe"""
    dados = DadosProcesso(sistema="pje")
    
    # N√∫mero do processo
    match = re.search(r'N√∫mero:\s*([\d.-]+)', texto)
    if match:
        dados.numero = match.group(1).strip()
    
    # Classe
    match = re.search(r'Classe:\s*\[?\w*\]?\s*([^\n]+)', texto)
    if match:
        dados.classe = match.group(1).strip()
    
    # √ìrg√£o julgador
    match = re.search(r'√ìrg√£o julgador:\s*([^\n]+)', texto)
    if match:
        dados.vara = match.group(1).strip()
    
    # Valor da causa
    match = re.search(r'Valor da causa:\s*R?\$?\s*([\d.,]+)', texto)
    if match:
        dados.valor_causa = f"R$ {match.group(1).strip()}"
    
    # Data distribui√ß√£o
    match = re.search(r'(?:√öltima )?[Dd]istribui√ß√£o\s*:?\s*(\d{2}/\d{2}/\d{4})', texto)
    if match:
        dados.data_distribuicao = match.group(1)
    
    # Assunto
    match = re.search(r'Assuntos?:\s*([^\n]+)', texto)
    if match:
        dados.assunto = match.group(1).strip()
    
    # Partes - busca na tabela
    # Padr√£o: NOME (TIPO) seguido opcionalmente de ADVOGADO
    partes_pattern = r'([A-Z√Å√â√ç√ì√ö√á√É√ï][A-Z√Å√â√ç√ì√ö√á√É√ï\s]+)\s*\((AUTOR|R√âU|R√â|REQUERENTE|REQUERIDO|APELANTE|APELADO)[^)]*\)'
    for match in re.finditer(partes_pattern, texto[:3000]):
        nome = match.group(1).strip()
        polo = match.group(2).strip()
        if len(nome) > 3:
            dados.partes.append({
                'nome': nome,
                'polo': 'Autor' if polo in ['AUTOR', 'REQUERENTE', 'APELANTE'] else 'R√©u',
                'advogado': ''
            })
    
    # Eventos/Documentos - busca na tabela do PJe
    # Padr√£o: ID | Data | Documento | Tipo
    evento_pattern = r'(\d{2}/\d{2}/\d{4}\s+\d{2}:\d{2})\s+([^\n]+?)\s+(Peti√ß√£o|Contesta√ß√£o|Senten√ßa|Despacho|Decis√£o|Certid√£o|Intima√ß√£o|Cita√ß√£o|Manifesta√ß√£o|Ac√≥rd√£o|Recurso|Laudo|Impugna√ß√£o|R√©plica)[^\n]*'
    for match in re.finditer(evento_pattern, texto, re.IGNORECASE):
        data = match.group(1).split()[0]  # S√≥ a data, sem hora
        descricao = match.group(2).strip()
        tipo = match.group(3).strip()
        
        dados.eventos.append(EventoProcessual(
            data=data,
            tipo=tipo,
            descricao=descricao[:100]
        ))
    
    return dados


def extrair_dados_eproc(texto: str) -> DadosProcesso:
    """Extrai dados de PDF do sistema e-Proc"""
    dados = DadosProcesso(sistema="eproc")
    
    # N√∫mero do processo
    match = re.search(r'Processo:\s*([\d.-]+)', texto)
    if match:
        dados.numero = match.group(1).strip()
    
    # Eventos - padr√£o e-Proc com p√°gina de separa√ß√£o
    evento_pattern = r'Evento\s+(\d+).*?Data:\s*(\d{2}/\d{2}/\d{4})[^\n]*.*?(?:Tipo|Documento):\s*([^\n]+)'
    for match in re.finditer(evento_pattern, texto, re.DOTALL | re.IGNORECASE):
        numero = match.group(1)
        data = match.group(2)
        tipo = match.group(3).strip()
        
        dados.eventos.append(EventoProcessual(
            data=data,
            tipo=tipo,
            descricao=f"Evento {numero}"
        ))
    
    return dados


def extrair_dados_generico(texto: str) -> DadosProcesso:
    """Extrai dados de PDF sem sistema identificado"""
    dados = DadosProcesso(sistema="generico")
    
    # Tenta encontrar n√∫mero de processo em v√°rios formatos
    patterns_processo = [
        r'(\d{7}-\d{2}\.\d{4}\.\d\.\d{2}\.\d{4})',  # CNJ
        r'Processo\s*(?:n[¬∫o.]?)?\s*([\d./-]+)',
        r'Autos\s*(?:n[¬∫o.]?)?\s*([\d./-]+)',
    ]
    
    for pattern in patterns_processo:
        match = re.search(pattern, texto[:2000])
        if match:
            dados.numero = match.group(1).strip()
            break
    
    # Valor da causa
    match = re.search(r'[Vv]alor\s+(?:da\s+)?[Cc]ausa[:\s]*R?\$?\s*([\d.,]+)', texto)
    if match:
        dados.valor_causa = f"R$ {match.group(1)}"
    
    return dados


# ============================================================================
# EXTRA√á√ÉO DE PDF
# ============================================================================

def extrair_texto_pdf(caminho: Path) -> Tuple[str, int]:
    """Extrai texto de um PDF. Retorna (texto, num_paginas)"""
    texto = []
    num_paginas = 0
    
    try:
        reader = PdfReader(str(caminho))
        num_paginas = len(reader.pages)
        
        for i, pagina in enumerate(reader.pages):
            texto_pagina = pagina.extract_text() or ""
            if texto_pagina.strip():
                texto.append(f"\n[P√ÅGINA {i+1}]\n{texto_pagina}")
    except Exception as e:
        print(f"Erro ao ler {caminho.name}: {e}")
        return "", 0
    
    return "\n".join(texto), num_paginas


def dividir_em_chunks(texto: str, config: Config, modo: str = "local") -> List[str]:
    """Divide texto em chunks para processamento"""
    # Usa chunk maior para cloud (tem mais contexto)
    if modo in ["google", "anthropic", "openai", "xai"]:
        chunk_size = config.chunk_size_cloud
    else:
        chunk_size = config.chunk_size_local
    
    max_chars = int(chunk_size * config.chars_per_token)
    chunks = []
    
    # Divide por p√°ginas
    paginas = re.split(r'\n\[P√ÅGINA \d+\]\n', texto)
    paginas = [p for p in paginas if p.strip()]
    
    chunk_atual = ""
    for pagina in paginas:
        if len(chunk_atual) + len(pagina) < max_chars:
            chunk_atual += "\n" + pagina
        else:
            if chunk_atual.strip():
                chunks.append(chunk_atual.strip())
            # Se uma √∫nica p√°gina for maior que max_chars, divide ela
            if len(pagina) > max_chars:
                # Divide em partes menores
                for i in range(0, len(pagina), max_chars):
                    chunks.append(pagina[i:i+max_chars])
                chunk_atual = ""
            else:
                chunk_atual = pagina
    
    if chunk_atual.strip():
        chunks.append(chunk_atual.strip())
    
    return chunks if chunks else [texto[:max_chars]]


# ============================================================================
# PROMPTS
# ============================================================================

PROMPT_EXTRACAO = """Voc√™ √© um assistente de extra√ß√£o de dados processuais. Analise o texto e extraia APENAS informa√ß√µes factuais, sem fazer an√°lises ou sugest√µes.

TEXTO DO PROCESSO:
{texto}

Extraia as seguintes informa√ß√µes em formato JSON (retorne APENAS o JSON, sem texto adicional):

{{
    "partes": [
        {{"nome": "Nome completo da parte", "polo": "Autor/R√©u"}}
    ],
    "objeto_acao": "Descri√ß√£o breve do que se trata a a√ß√£o (1-2 frases)",
    "resumo_fatos": "Narrativa dos fatos em par√°grafos bem formatados, com quebras de linha (\\n\\n) entre par√°grafos. Conte a hist√≥ria do processo de forma clara e cronol√≥gica.",
    "valores_relevantes": [
        {{"descricao": "...", "valor": "R$ ..."}}
    ],
    "pedidos": ["Pedido 1", "Pedido 2"],
    "decisoes": [
        {{"data": "dd/mm/aaaa", "tipo": "Despacho/Decis√£o/Senten√ßa", "conteudo": "Resumo do que foi decidido"}}
    ],
    "teses_autor": ["Tese 1", "Tese 2"],
    "teses_reu": ["Tese 1", "Tese 2"],
    "documentos_importantes": [
        {{"tipo": "Peti√ß√£o Inicial/Contesta√ß√£o/Senten√ßa/etc", "data": "dd/mm/aaaa", "parte": "Quem apresentou", "resumo": "Resumo do conte√∫do principal do documento"}}
    ],
    "historico_detalhado": [
        {{"data": "dd/mm/aaaa", "evento": "Tipo do evento", "descricao": "O que aconteceu de fato, quem fez, qual o conte√∫do resumido"}}
    ],
    "status_atual": "Fase processual atual"
}}

REGRAS IMPORTANTES:
- RETORNE APENAS O JSON, sem explica√ß√µes antes ou depois
- Extraia APENAS o que est√° expl√≠cito no texto
- N√ÉO invente informa√ß√µes
- N√ÉO fa√ßa an√°lises ou sugest√µes jur√≠dicas
- PARTES: Extraia da PETI√á√ÉO INICIAL. O autor √© quem prop√µe a a√ß√£o. Os r√©us s√£o contra quem a a√ß√£o √© proposta. Use os nomes completos das pessoas/empresas, n√£o use termos como "Contestante", "Requerido", etc.
- Datas no formato dd/mm/aaaa
- Se n√£o encontrar uma informa√ß√£o, deixe vazio ou null
- No resumo_fatos, use par√°grafos separados por \\n\\n para facilitar leitura
- Em valores_relevantes, inclua APENAS valores diretamente relacionados √† causa (valor da causa, valores cobrados, danos pedidos). N√ÉO inclua capital social de empresas, valor de cotas, sal√°rios, etc.
- Em documentos_importantes, foque nas pe√ßas processuais principais: peti√ß√£o inicial, contesta√ß√µes, r√©plicas, decis√µes, senten√ßas, laudos
- Em historico_detalhado, seja espec√≠fico: n√£o "Manifesta√ß√£o", mas "Manifesta√ß√£o do autor sobre cita√ß√£o"
- Seja conciso e objetivo"""


PROMPT_CONSOLIDACAO = """Voc√™ √© um assistente de s√≠ntese processual. Consolide as extra√ß√µes parciais abaixo em um √∫nico documento coerente.

EXTRA√á√ïES PARCIAIS:
{extracoes}

DADOS J√Å CONHECIDOS DO PROCESSO:
- N√∫mero: {numero}
- Sistema: {sistema}
- Eventos j√° identificados: {num_eventos}

Gere uma consolida√ß√£o em formato JSON (retorne APENAS o JSON, sem texto adicional):

{{
    "objeto_acao": "S√≠ntese clara do objeto da a√ß√£o em 1-2 frases",
    "resumo_fatos": "Narrativa cronol√≥gica dos fatos, bem formatada com par√°grafos separados por \\n\\n. M√°ximo 600 palavras. Conte a hist√≥ria de forma clara.",
    "partes_consolidadas": [
        {{"nome": "Nome completo", "polo": "Autor/R√©u"}}
    ],
    "valores_relevantes": [
        {{"descricao": "...", "valor": "R$ ..."}}
    ],
    "teses_autor": [...],
    "teses_reu": [...],
    "documentos_importantes": [
        {{"tipo": "...", "data": "...", "parte": "...", "resumo": "Resumo detalhado do documento (3-5 frases)"}}
    ],
    "decisoes_importantes": [...],
    "historico_resumido": [
        {{"data": "...", "descricao": "Descri√ß√£o clara do que aconteceu"}}
    ],
    "status_atual": "..."
}}

REGRAS:
- RETORNE APENAS O JSON, sem explica√ß√µes antes ou depois
- Elimine redund√¢ncias e informa√ß√µes duplicadas
- Mantenha apenas fatos confirmados
- N√ÉO fa√ßa an√°lises jur√≠dicas ou sugest√µes
- PARTES: Use nomes completos das pessoas/empresas. N√ÉO use termos gen√©ricos como "Contestante", "Requerido", "Primeiro R√©u". Use os nomes reais.
- Use par√°grafos no resumo_fatos (separados por \\n\\n)
- Em valores_relevantes, inclua APENAS: valor da causa, valores contratuais em disputa, valores de danos pedidos. EXCLUA: capital social, cotas de empresas, dados societ√°rios
- Em documentos_importantes, fa√ßa um resumo √∫til de cada pe√ßa principal
- Seja factual e objetivo"""


# ============================================================================
# FUN√á√ïES DE NORMALIZA√á√ÉO E LIMPEZA (v3.0)
# ============================================================================

def normalizar_nome(nome: str) -> str:
    """Normaliza nome de parte para evitar duplicatas por acento/caixa"""
    if not nome:
        return ""
    
    import unicodedata
    
    # Remove acentos
    nome_normalizado = unicodedata.normalize('NFKD', nome)
    nome_normalizado = ''.join(c for c in nome_normalizado if not unicodedata.combining(c))
    
    # Converte para mai√∫sculas
    nome_normalizado = nome_normalizado.upper().strip()
    
    # Remove varia√ß√µes comuns de sufixos empresariais
    sufixos = [' LTDA.', ' LTDA', ' S/A', ' S.A.', ' S.A', ' EPP', ' ME', ' EIRELI', ' SOCIEDADE SIMPLES']
    for sufixo in sufixos:
        nome_normalizado = nome_normalizado.replace(sufixo, '')
    
    # Remove espa√ßos extras
    nome_normalizado = ' '.join(nome_normalizado.split())
    
    return nome_normalizado


def parse_data_brasileira(data_str: str) -> tuple:
    """Converte data dd/mm/aaaa para tupla orden√°vel (aaaa, mm, dd)"""
    if not data_str:
        return (0, 0, 0)
    
    try:
        # Tenta formato dd/mm/aaaa
        partes = data_str.strip().split('/')
        if len(partes) == 3:
            dia, mes, ano = partes
            return (int(ano), int(mes), int(dia))
    except:
        pass
    
    return (0, 0, 0)


def is_evento_relevante(descricao: str) -> bool:
    """Verifica se um evento do hist√≥rico √© juridicamente relevante"""
    if not descricao:
        return False
    
    desc_lower = descricao.lower()
    
    # Lista de termos que indicam eventos IRRELEVANTES (ru√≠do)
    termos_irrelevantes = [
        'assinado eletronicamente',
        'assinatura eletr√¥nica',
        'documento assinado',
        'concluso para assinatura',
        'conclusos para',
        'remetido para',
        'juntada autom√°tica',
        'certid√£o de publica√ß√£o',
        'vista ao',
        'autos recebidos',
        'aguardando',
        'expediente forense',
        'n√£o houve expediente',
        'feriado',
        'recesso',
        'portaria conjunta',
    ]
    
    for termo in termos_irrelevantes:
        if termo in desc_lower:
            return False
    
    return True


def categorizar_evento(descricao: str) -> str:
    """Categoriza evento como 'processual' ou 'fatico'"""
    if not descricao:
        return "processual"
    
    desc_lower = descricao.lower()
    
    # Termos que indicam eventos F√ÅTICOS (n√£o processuais)
    termos_faticos = [
        'contrato', 'pagamento', 'pago', 'boleto', 'parcela',
        'protesto', 'negativa√ß√£o', 'serasa', 'spc', 'cadastro',
        'whatsapp', 'mensagem', 'email', 'notifica√ß√£o extrajudicial',
        'renegocia√ß√£o', 'acordo', 'tratamento', 'servi√ßo',
        'emiss√£o', 'vencimento', 'presta√ß√£o',
    ]
    
    for termo in termos_faticos:
        if termo in desc_lower:
            return "fatico"
    
    return "processual"


def deduplicar_valores(valores: List[Dict]) -> List[Dict]:
    """Remove valores duplicados de forma mais inteligente"""
    if not valores:
        return []
    
    valores_unicos = []
    valores_vistos = {}  # valor_numerico -> item completo
    
    for v in valores:
        if not isinstance(v, dict):
            continue
        
        desc = v.get("descricao", "").strip()
        valor = v.get("valor", "").strip()
        
        if not desc or not valor:
            continue
        
        # Extrai valor num√©rico para compara√ß√£o
        valor_num = re.sub(r'[^\d,.]', '', valor).replace('.', '').replace(',', '.')
        try:
            valor_float = float(valor_num) if valor_num else 0
        except:
            valor_float = 0
        
        # Chave composta: valor num√©rico + primeiras palavras da descri√ß√£o
        palavras_chave = ' '.join(desc.lower().split()[:3])
        chave = f"{valor_float:.2f}|{palavras_chave}"
        
        if chave not in valores_vistos:
            valores_vistos[chave] = v
            valores_unicos.append(v)
    
    return valores_unicos


def mesclar_extracoes(extracoes: List[Dict]) -> Dict:
    """Mescla m√∫ltiplas extra√ß√µes em uma √∫nica, via Python (sem IA) - v3.0"""
    
    resultado = {
        "partes": [],
        "objeto_acao": "",
        "resumo_fatos": "",
        "valores_relevantes": [],
        "pedidos": [],
        "pedidos_autor": [],
        "pedidos_reu": [],
        "pedidos_reconvencao": [],
        "preliminares": [],
        "decisoes": [],
        "teses_autor": [],
        "teses_reu": [],
        "documentos_importantes": [],
        "historico_processual": [],  # Atos do processo
        "historico_fatico": [],      # Linha do tempo dos fatos
        "status_atual": ""
    }
    
    # Sets para evitar duplicatas (usando nomes normalizados)
    partes_normalizadas = {}  # nome_normalizado -> dados originais
    pedidos_vistos = set()
    teses_autor_vistas = set()
    teses_reu_vistas = set()
    docs_vistos = set()
    historico_vistos = set()
    
    # Coleta todos os valores para deduplicar depois
    todos_valores = []
    
    for ext in extracoes:
        if not isinstance(ext, dict):
            continue
        
        # Objeto da a√ß√£o - pega o primeiro n√£o vazio
        if not resultado["objeto_acao"] and ext.get("objeto_acao"):
            resultado["objeto_acao"] = ext["objeto_acao"]
        
        # Resumo dos fatos - concatena se diferentes
        resumo = ext.get("resumo_fatos", "")
        if resumo and resumo not in resultado["resumo_fatos"]:
            if resultado["resumo_fatos"]:
                resultado["resumo_fatos"] += "\n\n" + resumo
            else:
                resultado["resumo_fatos"] = resumo
        
        # Partes - normaliza e evita duplicatas
        for p in ext.get("partes", []):
            if isinstance(p, dict):
                nome_original = p.get("nome", "").strip()
                if not nome_original or nome_original.upper() == "NONE":
                    continue
                
                nome_norm = normalizar_nome(nome_original)
                if nome_norm and nome_norm not in partes_normalizadas:
                    partes_normalizadas[nome_norm] = p
        
        # Valores - coleta todos para deduplicar depois
        for v in ext.get("valores_relevantes", []):
            if isinstance(v, dict):
                todos_valores.append(v)
        
        # Pedidos
        for p in ext.get("pedidos", []):
            if p and isinstance(p, str):
                p_lower = p.lower().strip()[:50]
                if p_lower not in pedidos_vistos:
                    pedidos_vistos.add(p_lower)
                    resultado["pedidos"].append(p)
        
        # Decis√µes
        for d in ext.get("decisoes", []):
            if isinstance(d, dict):
                resultado["decisoes"].append(d)
        
        # Teses do autor
        for t in ext.get("teses_autor", []):
            if t and isinstance(t, str):
                t_lower = t.lower().strip()[:50]
                if t_lower not in teses_autor_vistas:
                    teses_autor_vistas.add(t_lower)
                    resultado["teses_autor"].append(t)
        
        # Teses do r√©u
        for t in ext.get("teses_reu", []):
            if t and isinstance(t, str):
                t_lower = t.lower().strip()[:50]
                if t_lower not in teses_reu_vistas:
                    teses_reu_vistas.add(t_lower)
                    resultado["teses_reu"].append(t)
        
        # Documentos importantes
        for d in ext.get("documentos_importantes", []):
            if isinstance(d, dict):
                tipo = d.get("tipo", "").lower().strip()
                if tipo and tipo not in docs_vistos:
                    docs_vistos.add(tipo)
                    resultado["documentos_importantes"].append(d)
        
        # Hist√≥rico - filtra e categoriza
        for h in ext.get("historico_detalhado", []):
            if isinstance(h, dict):
                data = h.get("data", "")
                desc = h.get("descricao", h.get("evento", ""))
                
                # Pula eventos irrelevantes
                if not is_evento_relevante(desc):
                    continue
                
                chave = f"{data}|{desc[:30]}".lower()
                if chave not in historico_vistos:
                    historico_vistos.add(chave)
                    
                    # Categoriza e adiciona na lista correta
                    categoria = categorizar_evento(desc)
                    if categoria == "fatico":
                        resultado["historico_fatico"].append(h)
                    else:
                        resultado["historico_processual"].append(h)
        
        # Status - pega o √∫ltimo n√£o vazio
        if ext.get("status_atual"):
            resultado["status_atual"] = ext["status_atual"]
    
    # Finaliza partes (sem duplicatas)
    resultado["partes"] = list(partes_normalizadas.values())
    
    # Deduplica valores de forma inteligente
    resultado["valores_relevantes"] = deduplicar_valores(todos_valores)
    
    # Ordena hist√≥ricos por data (cronol√≥gico)
    for campo in ["historico_processual", "historico_fatico"]:
        try:
            resultado[campo].sort(key=lambda x: parse_data_brasileira(x.get("data", "")))
        except:
            pass
    
    # Mant√©m compatibilidade: hist√≥rico completo ordenado
    resultado["historico_detalhado"] = []
    resultado["historico_detalhado"].extend(resultado["historico_processual"])
    resultado["historico_detalhado"].extend(resultado["historico_fatico"])
    try:
        resultado["historico_detalhado"].sort(key=lambda x: parse_data_brasileira(x.get("data", "")))
    except:
        pass
    
    return resultado


# ============================================================================
# PROVEDORES DE LLM
# ============================================================================

def chamar_ollama(prompt: str, config: Config, timeout: int = 180) -> str:
    """Chama modelo local via Ollama"""
    try:
        r = requests.post(
            f"{config.ollama_host}/api/generate",
            json={
                "model": config.modelo_local,
                "prompt": prompt,
                "stream": False,
                "options": {"temperature": 0.2, "num_predict": 2000}
            },
            timeout=timeout
        )
        if r.status_code == 200:
            return r.json().get('response', '').strip()
    except Exception as e:
        print(f"Erro Ollama: {e}")
    return ""


# Controle de rate limiting para Google API gratuita
_google_last_request = 0
_google_request_count = 0
_google_minute_start = 0

def chamar_google(prompt: str, config: Config, retry_count: int = 0) -> str:
    """Chama Google Gemini API com rate limiting para plano gratuito"""
    global _google_last_request, _google_request_count, _google_minute_start
    
    if not config.api_google:
        raise ValueError("API key do Google n√£o configurada")
    
    # Rate limiting: m√°ximo 15 requisi√ß√µes por minuto no plano gratuito
    current_time = time.time()
    
    # Reset contador se passou 1 minuto
    if current_time - _google_minute_start > 60:
        _google_request_count = 0
        _google_minute_start = current_time
    
    # Se atingiu limite, espera
    if _google_request_count >= 14:  # 14 para margem de seguran√ßa
        wait_time = 60 - (current_time - _google_minute_start) + 1
        if wait_time > 0:
            print(f"    ‚è≥ Rate limit: aguardando {wait_time:.0f}s...")
            time.sleep(wait_time)
            _google_request_count = 0
            _google_minute_start = time.time()
    
    # Pausa m√≠nima entre requisi√ß√µes (4 segundos = ~15/min)
    time_since_last = current_time - _google_last_request
    if time_since_last < 4 and _google_last_request > 0:
        time.sleep(4 - time_since_last)
    
    # Modelo atualizado para 2026 (gemini-1.5-flash foi desativado em Set/2025)
    modelo = "gemini-2.5-flash"
    
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{modelo}:generateContent?key={config.api_google}"
    
    try:
        _google_last_request = time.time()
        _google_request_count += 1
        
        r = requests.post(
            url,
            json={
                "contents": [{"parts": [{"text": prompt}]}],
                "generationConfig": {
                    "temperature": 0.2,
                    "maxOutputTokens": 8000,
                    "responseMimeType": "application/json"  # For√ßa JSON v√°lido
                }
            },
            timeout=180  # Timeout maior para textos grandes
        )
        
        if r.status_code == 200:
            data = r.json()
            if 'candidates' in data and len(data['candidates']) > 0:
                return data['candidates'][0]['content']['parts'][0]['text']
            else:
                print(f"    ‚ö†Ô∏è Resposta vazia do Gemini")
                return ""
        
        elif r.status_code == 429:  # Rate limit exceeded
            if retry_count < 3:
                print(f"    ‚ö†Ô∏è Rate limit atingido, aguardando 60s... (tentativa {retry_count + 1}/3)")
                time.sleep(60)
                _google_request_count = 0
                _google_minute_start = time.time()
                return chamar_google(prompt, config, retry_count + 1)
            else:
                print(f"    ‚ùå Rate limit persistente ap√≥s 3 tentativas")
                return ""
        
        elif r.status_code == 400:
            error_msg = r.json().get('error', {}).get('message', r.text)
            print(f"    ‚ö†Ô∏è Erro 400: {error_msg[:100]}")
            # Se o prompt for muito grande, pode ser erro de tamanho
            if "too long" in error_msg.lower() or "token" in error_msg.lower():
                print(f"    üí° Texto pode ser muito grande, tente dividir o processo")
            return ""
        
        else:
            print(f"Erro Google API: {r.status_code} - {r.text[:200]}")
            
    except requests.exceptions.Timeout:
        print(f"    ‚ö†Ô∏è Timeout na requisi√ß√£o (180s)")
        if retry_count < 2:
            return chamar_google(prompt, config, retry_count + 1)
    except Exception as e:
        print(f"Erro Google: {e}")
    
    return ""


def chamar_anthropic(prompt: str, config: Config) -> str:
    """Chama Anthropic Claude API"""
    if not config.api_anthropic:
        raise ValueError("API key da Anthropic n√£o configurada")
    
    try:
        r = requests.post(
            "https://api.anthropic.com/v1/messages",
            headers={
                "x-api-key": config.api_anthropic,
                "anthropic-version": "2023-06-01",
                "content-type": "application/json"
            },
            json={
                "model": "claude-sonnet-4-20250514",
                "max_tokens": 4000,
                "messages": [{"role": "user", "content": prompt}]
            },
            timeout=120
        )
        if r.status_code == 200:
            data = r.json()
            return data['content'][0]['text']
        else:
            print(f"Erro Anthropic API: {r.status_code} - {r.text}")
    except Exception as e:
        print(f"Erro Anthropic: {e}")
    return ""


def chamar_openai(prompt: str, config: Config) -> str:
    """Chama OpenAI GPT API"""
    if not config.api_openai:
        raise ValueError("API key da OpenAI n√£o configurada")
    
    try:
        r = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {config.api_openai}",
                "Content-Type": "application/json"
            },
            json={
                "model": "gpt-4o",
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.2,
                "max_tokens": 4000
            },
            timeout=120
        )
        if r.status_code == 200:
            data = r.json()
            return data['choices'][0]['message']['content']
        else:
            print(f"Erro OpenAI API: {r.status_code} - {r.text}")
    except Exception as e:
        print(f"Erro OpenAI: {e}")
    return ""


def chamar_xai(prompt: str, config: Config) -> str:
    """Chama xAI Grok API"""
    if not config.api_xai:
        raise ValueError("API key da xAI n√£o configurada")
    
    try:
        r = requests.post(
            "https://api.x.ai/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {config.api_xai}",
                "Content-Type": "application/json"
            },
            json={
                "model": "grok-beta",
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.2,
                "max_tokens": 4000
            },
            timeout=120
        )
        if r.status_code == 200:
            data = r.json()
            return data['choices'][0]['message']['content']
        else:
            print(f"Erro xAI API: {r.status_code} - {r.text}")
    except Exception as e:
        print(f"Erro xAI: {e}")
    return ""


def chamar_llm(prompt: str, modo: str, config: Config) -> str:
    """Chama o LLM apropriado baseado no modo selecionado"""
    if modo == "local":
        return chamar_ollama(prompt, config)
    elif modo == "google":
        return chamar_google(prompt, config)
    elif modo == "anthropic":
        return chamar_anthropic(prompt, config)
    elif modo == "openai":
        return chamar_openai(prompt, config)
    elif modo == "xai":
        return chamar_xai(prompt, config)
    else:
        raise ValueError(f"Modo desconhecido: {modo}")


# ============================================================================
# PROCESSAMENTO PRINCIPAL
# ============================================================================

def processar_processo(pasta: Path, modo: str, config: Config, callback=None) -> Dict:
    """Processa todos os PDFs de uma pasta"""
    
    def log(msg):
        print(msg)
        if callback:
            callback(msg)
    
    resultado = {
        'dados': DadosProcesso(),
        'extracao': {},
        'tempo': 0,
        'modo': modo
    }
    
    inicio = time.time()
    
    # Encontra PDFs (incluindo subpastas)
    pdfs = list(pasta.glob("*.pdf"))
    pdfs += list(pasta.glob("**/*.pdf"))  # Subpastas tamb√©m
    pdfs = list(set(pdfs))  # Remove duplicatas de caminho
    
    if not pdfs:
        log("‚ùå Nenhum PDF encontrado!")
        return resultado
    
    log(f"üìÑ Encontrados {len(pdfs)} PDFs")
    
    # Identifica arquivos importantes (prefixo ou subpasta)
    prefixos_importantes = ("IMPORTANTE_", "PRINCIPAL_", "DESTAQUE_")
    pasta_importantes = "importantes"
    
    arquivos_importantes = []
    arquivos_normais = []
    
    for pdf in pdfs:
        eh_importante = (
            any(pdf.name.startswith(p) for p in prefixos_importantes) or
            pasta_importantes in str(pdf.parent).lower()
        )
        if eh_importante:
            arquivos_importantes.append(pdf)
            log(f"  ‚≠ê {pdf.name} (IMPORTANTE)")
        else:
            arquivos_normais.append(pdf)
            log(f"  üìÑ {pdf.name}")
    
    # Extrai texto de todos os PDFs e deduplica por conte√∫do
    textos_unicos = {}  # hash -> (nome, texto, importante)
    total_paginas = 0
    
    # Processa importantes primeiro
    for pdf in arquivos_importantes + arquivos_normais:
        texto, num_pag = extrair_texto_pdf(pdf)
        if texto.strip():
            # Gera hash do conte√∫do para detectar duplicatas
            texto_hash = hashlib.md5(texto[:10000].encode()).hexdigest()
            eh_importante = pdf in arquivos_importantes
            
            if texto_hash not in textos_unicos:
                textos_unicos[texto_hash] = (pdf.name, texto, eh_importante)
                total_paginas += num_pag
            else:
                # Se j√° existe, mas este √© importante e o outro n√£o, substitui
                nome_existente, texto_existente, importante_existente = textos_unicos[texto_hash]
                if eh_importante and not importante_existente:
                    textos_unicos[texto_hash] = (pdf.name, texto, True)
                    log(f"    ‚ö†Ô∏è Conte√∫do igual a '{nome_existente}', mas este √© IMPORTANTE - usando este")
                else:
                    log(f"    ‚ö†Ô∏è Conte√∫do duplicado de '{nome_existente}' - ignorando")
        else:
            log(f"    ‚ö†Ô∏è {pdf.name}: Sem texto extra√≠vel (verifique o OCR)")
    
    log(f"üìä Total: {total_paginas} p√°ginas ({len(textos_unicos)} documentos √∫nicos)")
    
    # Conta importantes
    num_importantes = sum(1 for _, _, imp in textos_unicos.values() if imp)
    if num_importantes > 0:
        log(f"‚≠ê {num_importantes} documentos marcados como importantes")
    
    if not textos_unicos:
        log("‚ùå Nenhum texto extra√≠do! Verifique o OCR.")
        return resultado
    
    # Junta todos os textos √∫nicos (importantes primeiro)
    textos_ordenados = sorted(textos_unicos.values(), key=lambda x: (not x[2], x[0]))  # Importantes primeiro
    texto_completo = "\n\n".join([t[1] for t in textos_ordenados])
    
    # Detecta sistema e extrai dados estruturados
    log("\nüîç Detectando sistema processual...")
    sistema = detectar_sistema(texto_completo)
    log(f"  Sistema identificado: {sistema.upper()}")
    
    if sistema == "pje":
        dados = extrair_dados_pje(texto_completo)
    elif sistema == "eproc":
        dados = extrair_dados_eproc(texto_completo)
    else:
        dados = extrair_dados_generico(texto_completo)
    
    # Deduplica eventos tamb√©m
    eventos_unicos = []
    eventos_vistos = set()
    for e in dados.eventos:
        chave = f"{e.data}|{e.tipo}|{e.descricao}"
        if chave not in eventos_vistos:
            eventos_vistos.add(chave)
            eventos_unicos.append(e)
    dados.eventos = eventos_unicos
    
    log(f"  Processo: {dados.numero or 'n√£o identificado'}")
    log(f"  Eventos encontrados: {len(dados.eventos)}")
    
    # Divide em chunks para an√°lise (tamanho depende do modo)
    chunks = dividir_em_chunks(texto_completo, config, modo)
    log(f"\nüìù Dividido em {len(chunks)} partes para an√°lise")
    
    # Se for cloud e tiver poucos chunks, pode processar tudo de uma vez
    if modo in ["google", "anthropic", "openai", "xai"] and len(chunks) <= 3:
        log(f"   üí° Contexto grande dispon√≠vel - processamento otimizado")
    
    # Extrai informa√ß√µes de cada chunk
    log(f"\nü§ñ Processando com {modo.upper()}...")
    extracoes = []
    
    for i, chunk in enumerate(chunks):
        log(f"  Parte {i+1}/{len(chunks)}...")
        
        prompt = PROMPT_EXTRACAO.format(texto=chunk[:15000])
        resposta = chamar_llm(prompt, modo, config)
        
        if resposta:
            # Tenta extrair JSON da resposta
            try:
                # Remove code blocks markdown se existirem
                resposta_limpa = resposta
                if '```json' in resposta_limpa:
                    resposta_limpa = re.sub(r'```json\s*', '', resposta_limpa)
                    resposta_limpa = re.sub(r'```\s*$', '', resposta_limpa)
                elif '```' in resposta_limpa:
                    resposta_limpa = re.sub(r'```\s*', '', resposta_limpa)
                
                # Procura por JSON na resposta
                json_match = re.search(r'\{[\s\S]*\}', resposta_limpa)
                if json_match:
                    json_str = json_match.group()
                    extracao = json.loads(json_str)
                    extracoes.append(extracao)
                    log(f"    ‚úÖ Extra√≠do com sucesso")
                else:
                    log(f"    ‚ö†Ô∏è Nenhum JSON encontrado na resposta")
                    # Log primeiros 200 chars para debug
                    log(f"    üìù In√≠cio da resposta: {resposta[:200]}...")
            except json.JSONDecodeError as e:
                log(f"    ‚ö†Ô∏è Resposta n√£o √© JSON v√°lido: {str(e)[:50]}")
                # Tenta corrigir erros comuns de JSON
                try:
                    json_str = resposta
                    # Remove texto antes/depois do JSON
                    inicio_json = json_str.find('{')
                    fim_json = json_str.rfind('}')
                    if inicio_json >= 0 and fim_json > inicio_json:
                        json_str = json_str[inicio_json:fim_json+1]
                    
                    # Corre√ß√µes comuns de JSON malformado
                    # 1. Remove v√≠rgulas antes de } ou ]
                    json_str = re.sub(r',\s*}', '}', json_str)
                    json_str = re.sub(r',\s*]', ']', json_str)
                    # 2. Adiciona v√≠rgulas faltando entre } e "
                    json_str = re.sub(r'}\s*"', '}, "', json_str)
                    json_str = re.sub(r']\s*"', '], "', json_str)
                    # 3. Corrige aspas n√£o escapadas dentro de strings (mais complexo)
                    # 4. Remove quebras de linha dentro de strings
                    json_str = re.sub(r'(?<!\\)\n', ' ', json_str)
                    
                    extracao = json.loads(json_str)
                    extracoes.append(extracao)
                    log(f"    ‚úÖ Extra√≠do ap√≥s corre√ß√£o autom√°tica")
                except Exception as e2:
                    log(f"    ‚ùå Falha definitiva no parse: {str(e2)[:30]}")
    
    # Consolida extra√ß√µes via Python (mais r√°pido e confi√°vel que IA)
    if len(extracoes) > 1:
        log("\nüìã Consolidando informa√ß√µes...")
        resultado['extracao'] = mesclar_extracoes(extracoes)
        log("    ‚úÖ Consolida√ß√£o OK")
    elif extracoes:
        resultado['extracao'] = extracoes[0]
    elif not extracoes:
        log("    ‚ö†Ô∏è Nenhuma extra√ß√£o bem-sucedida - relat√≥rio pode ficar incompleto")
    
    resultado['dados'] = dados
    resultado['tempo'] = time.time() - inicio
    
    log(f"\n‚úÖ Conclu√≠do em {resultado['tempo']:.1f} segundos")
    
    return resultado


# ============================================================================
# GERA√á√ÉO DE RELAT√ìRIOS
# ============================================================================

def gerar_markdown(resultado: Dict, pasta: Path) -> str:
    """Gera relat√≥rio em Markdown"""
    dados = resultado['dados']
    extracao = resultado.get('extracao', {})
    
    md = []
    
    # Cabe√ßalho
    md.append("# S√≠ntese Processual")
    md.append(f"**Processo:** {dados.numero or 'N√£o identificado'}")
    md.append(f"**Gerado em:** {datetime.now().strftime('%d/%m/%Y √†s %H:%M')}")
    md.append(f"**Modo:** {resultado.get('modo', 'N/D').upper()}")
    md.append(f"**Tempo de processamento:** {resultado.get('tempo', 0):.1f} segundos")
    md.append("")
    md.append("---")
    md.append("")
    
    # Dados Gerais
    md.append("## Dados Gerais")
    md.append("")
    if dados.classe:
        md.append(f"- **Classe:** {dados.classe}")
    if dados.vara:
        md.append(f"- **Vara:** {dados.vara}")
    if dados.valor_causa:
        md.append(f"- **Valor da causa:** {dados.valor_causa}")
    if dados.data_distribuicao:
        md.append(f"- **Distribui√ß√£o:** {dados.data_distribuicao}")
    if dados.assunto:
        md.append(f"- **Assunto:** {dados.assunto}")
    md.append("")
    
    # Partes
    md.append("## Partes")
    md.append("")
    
    partes = extracao.get('partes_consolidadas') or extracao.get('partes') or dados.partes
    if partes:
        md.append("| Polo | Nome |")
        md.append("|------|------|")
        for p in partes:
            if isinstance(p, dict):
                nome = p.get('nome', 'N/D')
                if nome and nome != 'None' and nome != 'null':
                    md.append(f"| {p.get('polo', 'N/D')} | {nome} |")
        md.append("")
    
    # Objeto da A√ß√£o
    objeto = extracao.get('objeto_acao', '')
    if objeto:
        md.append("## Objeto da A√ß√£o")
        md.append("")
        md.append(objeto)
        md.append("")
    
    # Resumo dos Fatos (com par√°grafos)
    resumo = extracao.get('resumo_fatos', '')
    
    if resumo:
        md.append("## Resumo dos Fatos")
        md.append("")
        # Garante que h√° quebras de par√°grafo
        resumo_formatado = resumo.replace('\\n\\n', '\n\n').replace('\\n', '\n')
        # Se n√£o tem par√°grafos, tenta dividir em senten√ßas longas
        if '\n\n' not in resumo_formatado and len(resumo_formatado) > 500:
            # Divide em par√°grafos a cada ~300 caracteres no ponto final
            partes = []
            atual = ""
            for frase in resumo_formatado.split('. '):
                atual += frase + '. '
                if len(atual) > 300:
                    partes.append(atual.strip())
                    atual = ""
            if atual.strip():
                partes.append(atual.strip())
            resumo_formatado = '\n\n'.join(partes)
        md.append(resumo_formatado)
        md.append("")
    
    # Documentos Importantes (NOVA SE√á√ÉO)
    docs_importantes = extracao.get('documentos_importantes', [])
    if docs_importantes:
        md.append("## Documentos Importantes")
        md.append("")
        for i, doc in enumerate(docs_importantes, 1):
            if isinstance(doc, dict):
                tipo = doc.get('tipo', 'Documento')
                data = doc.get('data', '')
                parte = doc.get('parte', '')
                resumo_doc = doc.get('resumo', '')
                
                titulo = f"### {i}. {tipo}"
                if data:
                    titulo += f" ({data})"
                md.append(titulo)
                if parte:
                    md.append(f"**Apresentado por:** {parte}")
                md.append("")
                if resumo_doc:
                    md.append(resumo_doc)
                md.append("")
        md.append("---")
        md.append("")
    
    # Hist√≥rico Processual (atos do processo)
    historico_proc = extracao.get('historico_processual', [])
    historico_geral = extracao.get('historico_resumido') or extracao.get('historico_detalhado', [])
    
    # Se tiver hist√≥rico processual separado, usa ele
    if historico_proc:
        md.append("## Hist√≥rico Processual")
        md.append("")
        md.append("| Data | Descri√ß√£o |")
        md.append("|------|-----------|")
        for h in historico_proc:
            if isinstance(h, dict):
                data = h.get('data', 'N/D')
                desc = h.get('descricao', h.get('evento', 'N/D'))
                md.append(f"| {data} | {desc} |")
        md.append("")
    elif historico_geral:
        md.append("## Hist√≥rico Processual")
        md.append("")
        md.append("| Data | Descri√ß√£o |")
        md.append("|------|-----------|")
        for h in historico_geral:
            if isinstance(h, dict):
                data = h.get('data', 'N/D')
                desc = h.get('descricao', h.get('evento', 'N/D'))
                md.append(f"| {data} | {desc} |")
        md.append("")
    elif dados.eventos:
        md.append("## Hist√≥rico Processual")
        md.append("")
        md.append("| Data | Tipo | Descri√ß√£o |")
        md.append("|------|------|-----------|")
        for e in dados.eventos[:30]:
            md.append(f"| {e.data} | {e.tipo} | {e.descricao[:60]} |")
        md.append("")
    
    # Linha do Tempo F√°tica (se houver)
    historico_fatico = extracao.get('historico_fatico', [])
    if historico_fatico:
        md.append("## Linha do Tempo dos Fatos")
        md.append("")
        md.append("| Data | Descri√ß√£o |")
        md.append("|------|-----------|")
        for h in historico_fatico:
            if isinstance(h, dict):
                data = h.get('data', 'N/D')
                desc = h.get('descricao', h.get('evento', 'N/D'))
                md.append(f"| {data} | {desc} |")
        md.append("")
    
    # Valores
    valores = extracao.get('valores_relevantes') or extracao.get('valores_consolidados') or extracao.get('valores', [])
    if valores:
        md.append("## Valores Identificados")
        md.append("")
        for v in valores:
            if isinstance(v, dict):
                md.append(f"- **{v.get('descricao', 'N/D')}:** {v.get('valor', 'N/D')}")
        md.append("")
    
    # Teses das Partes
    teses_autor = extracao.get('teses_autor', [])
    teses_reu = extracao.get('teses_reu', [])
    
    if teses_autor or teses_reu:
        md.append("## Teses das Partes")
        md.append("")
        
        if teses_autor:
            md.append("**Autor:**")
            for t in teses_autor:
                md.append(f"- {t}")
            md.append("")
        
        if teses_reu:
            md.append("**R√©u:**")
            for t in teses_reu:
                md.append(f"- {t}")
            md.append("")
    
    # Decis√µes
    decisoes = extracao.get('decisoes_importantes') or extracao.get('decisoes', [])
    if decisoes:
        md.append("## Decis√µes")
        md.append("")
        for d in decisoes:
            if isinstance(d, dict):
                data = d.get('data', 'N/D')
                tipo = d.get('tipo', 'N/D')
                conteudo = d.get('conteudo', 'N/D')
                if conteudo and conteudo != 'None' and conteudo != 'null':
                    md.append(f"- **{data} - {tipo}:** {conteudo}")
        md.append("")
    
    # Status
    status = extracao.get('status_atual', '')
    if status:
        md.append("## Status Atual")
        md.append("")
        md.append(status)
        md.append("")
    
    # Rodap√©
    md.append("---")
    md.append("")
    md.append("*Documento gerado automaticamente pelo BotS√≠ntese v3.0*")
    md.append("*Este √© um resumo factual. N√£o cont√©m an√°lises ou recomenda√ß√µes jur√≠dicas.*")
    
    return "\n".join(md)


def gerar_docx(resultado: Dict, pasta: Path) -> Document:
    """Gera relat√≥rio em Word"""
    doc = Document()
    dados = resultado['dados']
    extracao = resultado.get('extracao', {})
    
    # T√≠tulo
    titulo = doc.add_heading('S√≠ntese Processual', 0)
    titulo.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    # Metadados
    doc.add_paragraph(f"Processo: {dados.numero or 'N√£o identificado'}")
    doc.add_paragraph(f"Gerado em: {datetime.now().strftime('%d/%m/%Y √†s %H:%M')}")
    doc.add_paragraph(f"Modo: {resultado.get('modo', 'N/D').upper()}")
    
    doc.add_paragraph("‚îÄ" * 50)
    
    # Dados Gerais
    doc.add_heading('Dados Gerais', level=1)
    if dados.classe:
        doc.add_paragraph(f"Classe: {dados.classe}")
    if dados.vara:
        doc.add_paragraph(f"Vara: {dados.vara}")
    if dados.valor_causa:
        doc.add_paragraph(f"Valor da causa: {dados.valor_causa}")
    if dados.data_distribuicao:
        doc.add_paragraph(f"Distribui√ß√£o: {dados.data_distribuicao}")
    
    # Partes
    doc.add_heading('Partes', level=1)
    partes = extracao.get('partes_consolidadas') or extracao.get('partes') or dados.partes
    if partes:
        table = doc.add_table(rows=1, cols=2)
        table.style = 'Table Grid'
        hdr = table.rows[0].cells
        hdr[0].text = 'Polo'
        hdr[1].text = 'Nome'
        for p in partes:
            if isinstance(p, dict):
                nome = p.get('nome', 'N/D')
                if nome and nome != 'None' and nome != 'null':
                    row = table.add_row().cells
                    row[0].text = str(p.get('polo', 'N/D') or 'N/D')
                    row[1].text = str(nome or 'N/D')
    
    # Objeto
    objeto = extracao.get('objeto_acao', '')
    if objeto:
        doc.add_heading('Objeto da A√ß√£o', level=1)
        doc.add_paragraph(objeto)
    
    # Resumo dos Fatos (com par√°grafos)
    resumo = extracao.get('resumo_fatos', '')
    if resumo:
        doc.add_heading('Resumo dos Fatos', level=1)
        # Formata par√°grafos
        resumo_formatado = resumo.replace('\\n\\n', '\n\n').replace('\\n', '\n')
        for paragrafo in resumo_formatado.split('\n\n'):
            if paragrafo.strip():
                doc.add_paragraph(paragrafo.strip())
    
    # Documentos Importantes
    docs_importantes = extracao.get('documentos_importantes', [])
    if docs_importantes:
        doc.add_heading('Documentos Importantes', level=1)
        for i, doc_imp in enumerate(docs_importantes, 1):
            if isinstance(doc_imp, dict):
                tipo = doc_imp.get('tipo', 'Documento')
                data = doc_imp.get('data', '')
                parte = doc_imp.get('parte', '')
                resumo_doc = doc_imp.get('resumo', '')
                
                titulo_doc = f"{i}. {tipo}"
                if data:
                    titulo_doc += f" ({data})"
                doc.add_heading(titulo_doc, level=2)
                if parte:
                    doc.add_paragraph(f"Apresentado por: {parte}")
                if resumo_doc:
                    doc.add_paragraph(resumo_doc)
    
    # Hist√≥rico Processual
    historico_proc = extracao.get('historico_processual', [])
    historico_geral = extracao.get('historico_resumido') or extracao.get('historico_detalhado', [])
    
    if historico_proc:
        doc.add_heading('Hist√≥rico Processual', level=1)
        table = doc.add_table(rows=1, cols=2)
        table.style = 'Table Grid'
        hdr = table.rows[0].cells
        hdr[0].text = 'Data'
        hdr[1].text = 'Descri√ß√£o'
        for h in historico_proc:
            if isinstance(h, dict):
                row = table.add_row().cells
                row[0].text = str(h.get('data', 'N/D') or 'N/D')
                row[1].text = str(h.get('descricao', h.get('evento', 'N/D')) or 'N/D')
    elif historico_geral:
        doc.add_heading('Hist√≥rico Processual', level=1)
        table = doc.add_table(rows=1, cols=2)
        table.style = 'Table Grid'
        hdr = table.rows[0].cells
        hdr[0].text = 'Data'
        hdr[1].text = 'Descri√ß√£o'
        for h in historico_geral:
            if isinstance(h, dict):
                row = table.add_row().cells
                row[0].text = str(h.get('data', 'N/D') or 'N/D')
                row[1].text = str(h.get('descricao', h.get('evento', 'N/D')) or 'N/D')
    elif dados.eventos:
        doc.add_heading('Hist√≥rico Processual', level=1)
        table = doc.add_table(rows=1, cols=3)
        table.style = 'Table Grid'
        hdr = table.rows[0].cells
        hdr[0].text = 'Data'
        hdr[1].text = 'Tipo'
        hdr[2].text = 'Descri√ß√£o'
        for e in dados.eventos[:30]:
            row = table.add_row().cells
            row[0].text = str(e.data or 'N/D')
            row[1].text = str(e.tipo or 'N/D')
            row[2].text = str(e.descricao[:50] if e.descricao else 'N/D')
    
    # Linha do Tempo F√°tica (se houver)
    historico_fatico = extracao.get('historico_fatico', [])
    if historico_fatico:
        doc.add_heading('Linha do Tempo dos Fatos', level=1)
        table = doc.add_table(rows=1, cols=2)
        table.style = 'Table Grid'
        hdr = table.rows[0].cells
        hdr[0].text = 'Data'
        hdr[1].text = 'Descri√ß√£o'
        for h in historico_fatico:
            if isinstance(h, dict):
                row = table.add_row().cells
                row[0].text = str(h.get('data', 'N/D') or 'N/D')
                row[1].text = str(h.get('descricao', h.get('evento', 'N/D')) or 'N/D')
    
    # Valores
    valores = extracao.get('valores_relevantes') or extracao.get('valores_consolidados', [])
    if valores:
        doc.add_heading('Valores Identificados', level=1)
        for v in valores:
            if isinstance(v, dict):
                doc.add_paragraph(f"‚Ä¢ {v.get('descricao', 'N/D')}: {v.get('valor', 'N/D')}")
    
    # Teses
    teses_autor = extracao.get('teses_autor', [])
    teses_reu = extracao.get('teses_reu', [])
    if teses_autor or teses_reu:
        doc.add_heading('Teses das Partes', level=1)
        if teses_autor:
            p = doc.add_paragraph()
            p.add_run("Autor:").bold = True
            for t in teses_autor:
                doc.add_paragraph(f"‚Ä¢ {t}")
        if teses_reu:
            p = doc.add_paragraph()
            p.add_run("R√©u:").bold = True
            for t in teses_reu:
                doc.add_paragraph(f"‚Ä¢ {t}")
    
    # Rodap√©
    doc.add_paragraph()
    doc.add_paragraph("‚îÄ" * 50)
    p = doc.add_paragraph()
    p.add_run("Documento gerado automaticamente pelo BotS√≠ntese v3.0").italic = True
    
    return doc


# ============================================================================
# INTERFACE GR√ÅFICA
# ============================================================================

class BotSinteseGUI:
    def __init__(self):
        self.root = tk.Tk()
        self.root.title("BotS√≠ntese v3.0 - S√≠ntese Processual")
        self.root.geometry("700x550")
        self.root.resizable(True, True)
        
        self.pasta_script = Path(__file__).parent
        self.config = carregar_config(self.pasta_script)
        self.pasta_selecionada = None
        self.processando = False
        
        self.criar_widgets()
    
    def criar_widgets(self):
        # Frame principal
        main = tk.Frame(self.root, padx=20, pady=15)
        main.pack(fill=tk.BOTH, expand=True)
        
        # T√≠tulo
        tk.Label(main, text="BotS√≠ntese v3.0", font=("Arial", 22, "bold")).pack()
        tk.Label(main, text="S√≠ntese Processual Automatizada", font=("Arial", 11)).pack()
        
        # Sele√ß√£o de pasta
        frame_pasta = tk.Frame(main)
        frame_pasta.pack(pady=15, fill=tk.X)
        
        self.btn_pasta = tk.Button(
            frame_pasta, text="üìÅ Selecionar Pasta do Processo",
            command=self.selecionar_pasta, font=("Arial", 11), padx=15, pady=8
        )
        self.btn_pasta.pack()
        
        self.lbl_pasta = tk.Label(frame_pasta, text="Nenhuma pasta selecionada", fg="gray")
        self.lbl_pasta.pack(pady=5)
        
        # Sele√ß√£o de modo
        frame_modo = tk.LabelFrame(main, text="Modo de Processamento", padx=15, pady=10)
        frame_modo.pack(pady=10, fill=tk.X)
        
        self.modo_var = tk.StringVar(value=self.config.modo_padrao)
        
        modos = [
            ("google", "‚òÅÔ∏è Google Gemini - GRATUITO (recomendado)"),
            ("local", "üñ•Ô∏è Local (Ollama) - Gratuito, mais lento"),
            ("anthropic", "‚òÅÔ∏è Anthropic Claude - ~R$ 1-3/processo"),
            ("openai", "‚òÅÔ∏è OpenAI GPT-4o - ~R$ 2-5/processo"),
            ("xai", "‚òÅÔ∏è xAI Grok - ~R$ 1-3/processo"),
        ]
        
        for valor, texto in modos:
            rb = tk.Radiobutton(
                frame_modo, text=texto, variable=self.modo_var, value=valor,
                font=("Arial", 10), anchor="w"
            )
            rb.pack(fill=tk.X)
        
        # Bot√£o configurar APIs
        tk.Button(
            frame_modo, text="‚öôÔ∏è Configurar APIs",
            command=self.abrir_config, font=("Arial", 9)
        ).pack(pady=5)
        
        # Bot√£o processar
        self.btn_processar = tk.Button(
            main, text="‚ñ∂Ô∏è Gerar S√≠ntese", command=self.iniciar_processamento,
            font=("Arial", 13, "bold"), padx=25, pady=12, state=tk.DISABLED
        )
        self.btn_processar.pack(pady=10)
        
        # Log
        frame_log = tk.Frame(main)
        frame_log.pack(fill=tk.BOTH, expand=True, pady=5)
        
        self.log_text = tk.Text(frame_log, height=10, font=("Consolas", 9))
        self.log_text.pack(fill=tk.BOTH, expand=True, side=tk.LEFT)
        
        scrollbar = tk.Scrollbar(frame_log, command=self.log_text.yview)
        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        self.log_text.config(yscrollcommand=scrollbar.set)
        
        # Status
        self.lbl_status = tk.Label(main, text="Pronto", font=("Arial", 10))
        self.lbl_status.pack()
    
    def log(self, msg):
        self.log_text.insert(tk.END, msg + "\n")
        self.log_text.see(tk.END)
        self.root.update_idletasks()
    
    def selecionar_pasta(self):
        pasta = filedialog.askdirectory(title="Selecione a pasta com os PDFs")
        if pasta:
            self.pasta_selecionada = Path(pasta)
            self.lbl_pasta.config(text=str(self.pasta_selecionada), fg="black")
            self.btn_processar.config(state=tk.NORMAL)
            
            pdfs = list(self.pasta_selecionada.glob("*.pdf"))
            self.log(f"Pasta: {pasta}")
            self.log(f"PDFs encontrados: {len(pdfs)}")
    
    def abrir_config(self):
        """Abre janela de configura√ß√£o de APIs"""
        win = tk.Toplevel(self.root)
        win.title("Configurar APIs")
        win.geometry("500x400")
        win.transient(self.root)
        
        frame = tk.Frame(win, padx=20, pady=20)
        frame.pack(fill=tk.BOTH, expand=True)
        
        tk.Label(frame, text="Configura√ß√£o de APIs", font=("Arial", 14, "bold")).pack()
        tk.Label(frame, text="Preencha apenas as APIs que deseja usar", font=("Arial", 9)).pack(pady=5)
        
        # Campos
        campos = [
            ("Google (Gemini):", "api_google"),
            ("Anthropic (Claude):", "api_anthropic"),
            ("OpenAI (GPT):", "api_openai"),
            ("xAI (Grok):", "api_xai"),
        ]
        
        entries = {}
        for label, attr in campos:
            f = tk.Frame(frame)
            f.pack(fill=tk.X, pady=5)
            tk.Label(f, text=label, width=18, anchor="e").pack(side=tk.LEFT)
            e = tk.Entry(f, width=45, show="*")
            e.insert(0, getattr(self.config, attr, ''))
            e.pack(side=tk.LEFT, padx=5)
            entries[attr] = e
        
        # Ollama
        tk.Label(frame, text="‚îÄ" * 40).pack(pady=10)
        tk.Label(frame, text="Configura√ß√£o Local (Ollama)", font=("Arial", 11, "bold")).pack()
        
        f = tk.Frame(frame)
        f.pack(fill=tk.X, pady=5)
        tk.Label(f, text="Host:", width=18, anchor="e").pack(side=tk.LEFT)
        e_host = tk.Entry(f, width=45)
        e_host.insert(0, self.config.ollama_host)
        e_host.pack(side=tk.LEFT, padx=5)
        
        f = tk.Frame(frame)
        f.pack(fill=tk.X, pady=5)
        tk.Label(f, text="Modelo:", width=18, anchor="e").pack(side=tk.LEFT)
        e_modelo = tk.Entry(f, width=45)
        e_modelo.insert(0, self.config.modelo_local)
        e_modelo.pack(side=tk.LEFT, padx=5)
        
        def salvar():
            for attr, entry in entries.items():
                setattr(self.config, attr, entry.get().strip())
            self.config.ollama_host = e_host.get().strip()
            self.config.modelo_local = e_modelo.get().strip()
            salvar_config(self.config, self.pasta_script)
            messagebox.showinfo("Salvo", "Configura√ß√µes salvas!")
            win.destroy()
        
        tk.Button(frame, text="üíæ Salvar", command=salvar, font=("Arial", 11), padx=20).pack(pady=15)
    
    def iniciar_processamento(self):
        if self.processando:
            return
        
        modo = self.modo_var.get()
        
        # Verifica se API est√° configurada
        if modo == "google" and not self.config.api_google:
            messagebox.showerror("Erro", "API do Google n√£o configurada!\nClique em 'Configurar APIs'.")
            return
        elif modo == "anthropic" and not self.config.api_anthropic:
            messagebox.showerror("Erro", "API da Anthropic n√£o configurada!")
            return
        elif modo == "openai" and not self.config.api_openai:
            messagebox.showerror("Erro", "API da OpenAI n√£o configurada!")
            return
        elif modo == "xai" and not self.config.api_xai:
            messagebox.showerror("Erro", "API da xAI n√£o configurada!")
            return
        
        self.processando = True
        self.btn_processar.config(state=tk.DISABLED)
        self.btn_pasta.config(state=tk.DISABLED)
        self.lbl_status.config(text="Processando...")
        
        # Salva modo como padr√£o
        self.config.modo_padrao = modo
        salvar_config(self.config, self.pasta_script)
        
        thread = threading.Thread(target=self.processar, args=(modo,))
        thread.start()
    
    def processar(self, modo):
        try:
            self.log(f"\n{'='*50}")
            self.log(f"Iniciando processamento em modo {modo.upper()}")
            self.log(f"{'='*50}\n")
            
            # Verifica Ollama se modo local
            if modo == "local":
                self.log("üîå Verificando Ollama...")
                try:
                    r = requests.get(f"{self.config.ollama_host}/api/tags", timeout=5)
                    if r.status_code != 200:
                        raise Exception("Ollama n√£o respondeu")
                    self.log("‚úÖ Ollama conectado")
                except:
                    self.log("‚ùå Ollama n√£o est√° rodando!")
                    self.log("Execute no WSL2: ollama serve")
                    messagebox.showerror("Erro", "Ollama n√£o est√° rodando!")
                    return
            
            # Processa
            resultado = processar_processo(
                self.pasta_selecionada, modo, self.config, self.log
            )
            
            if not resultado['dados'].numero and not resultado['extracao']:
                messagebox.showwarning("Aviso", "N√£o foi poss√≠vel extrair dados do processo!")
                return
            
            # Gera relat√≥rios
            self.log("\nüìù Gerando relat√≥rios...")
            
            # Markdown (com BOM para melhor compatibilidade Windows)
            md_content = gerar_markdown(resultado, self.pasta_selecionada)
            md_path = self.pasta_selecionada / "sintese_processual.md"
            with open(md_path, 'w', encoding='utf-8-sig') as f:
                f.write(md_content)
            self.log(f"  ‚úÖ {md_path.name}")
            
            # Word
            docx_doc = gerar_docx(resultado, self.pasta_selecionada)
            docx_path = self.pasta_selecionada / "sintese_processual.docx"
            docx_doc.save(docx_path)
            self.log(f"  ‚úÖ {docx_path.name}")
            
            # Resultado
            tempo = resultado.get('tempo', 0)
            self.log(f"\n‚úÖ Conclu√≠do em {tempo:.1f} segundos!")
            
            messagebox.showinfo(
                "Conclu√≠do!",
                f"S√≠ntese gerada em {tempo:.1f} segundos!\n\n"
                f"Arquivos salvos em:\n{self.pasta_selecionada}"
            )
            
            os.startfile(self.pasta_selecionada)
            
        except Exception as e:
            self.log(f"\n‚ùå Erro: {e}")
            import traceback
            self.log(traceback.format_exc())
            messagebox.showerror("Erro", str(e))
        
        finally:
            self.processando = False
            self.root.after(0, lambda: self.btn_processar.config(state=tk.NORMAL))
            self.root.after(0, lambda: self.btn_pasta.config(state=tk.NORMAL))
            self.root.after(0, lambda: self.lbl_status.config(text="Pronto"))
    
    def executar(self):
        self.root.mainloop()


# ============================================================================
# MAIN
# ============================================================================

if __name__ == "__main__":
    if len(sys.argv) > 1:
        # Modo CLI
        pasta = Path(sys.argv[1])
        modo = sys.argv[2] if len(sys.argv) > 2 else "local"
        
        if not pasta.exists():
            print(f"Erro: Pasta n√£o encontrada: {pasta}")
            sys.exit(1)
        
        config = carregar_config(Path(__file__).parent)
        resultado = processar_processo(pasta, modo, config)
        
        if resultado['dados'].numero or resultado['extracao']:
            md = gerar_markdown(resultado, pasta)
            (pasta / "sintese_processual.md").write_text(md, encoding='utf-8')
            print(f"\n‚úÖ S√≠ntese salva em: {pasta / 'sintese_processual.md'}")
    else:
        # Modo GUI
        gui = BotSinteseGUI()
        gui.executar()
